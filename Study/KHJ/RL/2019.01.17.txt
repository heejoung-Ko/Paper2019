Lec 04

Exploit vs Exploration
현재 값 이용 vs 모험&도전

E-greedy
e = 0.1
if rand < e:
	a = random		# Exploit
else:
	a = argmax(Q(s, a))	# Exploration

decaying E-greedy
: 학습하는 초기에는 랜덤한 값을 많이 받고, 후기에는 적게 받기
for i in range(1000)
	e = 0.1 / (i + 1)
	if rand < e:
		a = random		# Exploit
	else:
		a = argmax(Q(s, a))	# Exploration

add random noise
a = argmax(Q(s, a) + random_values)
+)
for i in range(1000)
	a = argmax(Q(s, a) + random_values / (i + 1))

discounted reward
: 나중에 받는 보상 일수록 가치가 떨어지게 설정 -> 보상을 빨리 받는 쪽으로 학습 (최단, 최적의 경로)
Q^(s, a) <- r + γ * max Q^(s', a')	# γ = 0.9

Q^가 완벽한 Q에 수렴하는가?
아래와 같은 조건이 있을 때 수렴
1) 어떤 방향으로 갈때 항상 같은 상을 받는다
2) 상태의 수가 유한하다

========================================================================================

Lab 04

