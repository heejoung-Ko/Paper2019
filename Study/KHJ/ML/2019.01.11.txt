Lec 12

RNN (Recurrent Neural Network)

sequence data
: 이전에 했던 연산이 다음 연산에 영향을 미쳐야 함

Ht = fw(Ht-1, Xt) (H는 state)

Ht  = tanh(WhhHt-1 + WxhXt)
yt = WhyHt

Language Modeling
Speech
Tanslation
bot
image/vedio caption

==============================================================================

Lab 12-1

1) cell 생성
cell = tf.contrib.rnnBasicRNNCell(num_units=hidden_size)

2) 학습시키기 (두개의 결과물을 뱉어냄)
outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)

hidden_size: 출력 값
sqeunce_length: cell을 몇개 펼칠 것인가, 입력 데이터에 따라 달라짐 ex) x_data = [h, e, l, l, o] 일시 5
batch_size: sqeunce 데이터를 몇개씩 넣을 것인가, 입력 데이터에 따라 달라짐
	    ex) shape = (3, 2, 1): [[[1, 0], [0, 1]],	
				 [[0, 0], [1, 1]], 
				 [[0, 1], [1, 0]]] #3 = batch_size, 2 = sqeunce_length

==============================================================================

Lab 12-2

sequence_loss(logits=예측값, targets=실제값, weights=weghit(보통 전부 1로 줌))
: 예측값과 실제값을 비교, 오차 계산

==============================================================================

Lab 12-3

Long Sequence를 작업할 때 자동화 하기

idx2char = list(set(sample))			# index -> char
char2dix = {c: i for i, c in enumerate(idx2char)}	# char -> index
-> one_hot(X, num_classes)

dic_size = hidden_size, num_classes = len(char2idx)
sqeunce_length = len(sample) - 1

