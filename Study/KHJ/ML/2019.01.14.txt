Lab 12-4

Deep & Wide하게 -> Stacked RNN

cell = rnn.BasicLSTMCell(hidden, state_is_tuple = True)
cell = rnn.MultiRNNCell([cell] * n, state_is_tuple = True) 
(n: 쌓고 싶은 만큼~)

CNN에서 후반에 softmax(fully connect)를 붙여서 더 좋은 결과를 얻음
RNN + softmax
reshape로 쌓아서 softmax로 보냄 -> 결과 값을 reshape로 펼침
1) 쌓기
X_for_softmax = tf.reshape(outputs, [-1, hidden_size])
2) 펼치기
outputs = tf.reshape(outputs, [batch_size, seq_length, num_classes])
3) 
sequence_loss = tf.contrib.seq2seq.sequence_loss(
		logits=outputs, targers=Y, weights=weights)
mean_loss = tf.reduce_mean(sequence_loss)
=============================================================================

Lab 12-5

Dynamic RNN
: 가변하는 sequence(문자열)를 학습할 수 있어야 함!
각각 batch에 문자열의 값을 함께 줌
sequence_length = [5, 3, 4]

outputs, _states = tf.nn.dynamic_rnn(
		cell, x_data, sequence_length = [5, 3, 4], dtype = tf.float32)

dynamic_rnn = sequence_length에 따라 없는 데이터의 값은 0으로 출력

=============================================================================

Lab 12-6

Time Series Data
: 시간에 따라 값이 가변하는 데이터

Many to one
: 1~n time의 데이터를 학습하여 n+1 time째의 데이터를 예측하는 것
이전의 데이터가 다음 데이터에 영향을 미친다


