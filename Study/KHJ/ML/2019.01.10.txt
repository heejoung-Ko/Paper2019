Lec 11-1

ConvNet (Convolutional Neural Networks)
: 여러개의 입력값을 하나로 합쳐 하나의 결과를 얻는 아이디어에서 출발

filter
일부의 크기로 나누어 입력 값을 받아 하나의 값으로 만들어 냄 (Wx + b, ReLU(Wx + b))
stride: filter가 한번에 움직이는 크기
input 이미지의 N x N, filter의 사이즈는 F x F 일때
output size: (N - F) / stride + 1

Pading
filter로 나오는 값의 집합은 원래 크기보다 작아짐 -> 데이터 손실
이를 방지하기 위해 pading 사용 
N x N -> (N+2) x (N+2) (추가된 공간의 데이터는 0으로 입력)
filter를 통해도 N x N의 출력값을 얻을 수 있음

여러 개의 필터를 통해 얻은 출력값을 Convolution Layer에 적용
activation maps (output_size, output_size, filters_num)
해당 작업을 반복

weight의 값은 filter_size(H) x filter_size(W) x filter_size(Depth) x filter_num

===============================================================================

Lec 11-2

Pooling Layer
conv layer에서 한 레이어를 뽑아 resize(sampling)
모든 레이어를 resize한 후 모은 것을 pooling layer

Max Pooling
filter 내에서 가장 큰 값만 뽑아내는 것

conv relu pool을 어떻게 쌓느냐는 마음대로
보통 맨 마지막에 pool 함
마지막에 나온 값을 classification 

===============================================================================

Lec 11-3

CNN 예
LeNEt-5, AlexNet, GoogLeNet, ResNet

===============================================================================

