모두를 위한 딥러닝 1

=====================================
Lec 00

=====================================
Lec 01

- Supervised learning (지도 학습)
  : label 되어 있는 데이터 셋으로 학습
- UnSupervised learning (비지도 학습)
  : label 되어 있지 않은 데이터 셋으로 학습

-

=====================================
Lab 01

constant: 노드 만들기
Session: 세션을 만들기
Session.run: 노드 실행

x = tf.placeholder(tf.float32), feed_dict={x: [3, 4], b: [2, 4]}: 여러 값 넣기

rank: 차원(열)
shape: 엘리먼트의 갯수(행)

=====================================
Lec 02

Linear regression

 H(x)  = Wx + b
(가설)

데이터 셋과 H(x) 그래프 사이에 거리를 계산 -> 거리가 적을 수록 좋은 가설

Cost(Loss) function: 거리 재는 함수
: (H(x) - y)^2 -> 차이가 클 때 더 많은 패널티
minimize cost(W, b) = 1/m * (( H(x(1)) - (y1) )^2 + ...) (시그마 1 ~ m)

=====================================
Lab 02

Variable:tensorflow가 자체적으로 변경시키는 값, run할 때 global_variable_initializer 해줘야 적용
reduce_mean: 평균 내주는 부분 (1/m * 시그마)

minimize
optimizer: learning_rate 설정 
train = optimizer.minimize(cost): cost를 자동으로 minimize 해줌

스텝이 돌면서 minimize 설정해둔 train을 run
내부적으로 W와 b 업데이트

=====================================
Lab 03

Gradient descent algorithm
: 임의의 값을 주고 경사를 구하고(미분) 경사를 타고 내려가서 cost가 0이 되는 지점을 찾는 algorithm

W := W - α 1/m ∑ ( Wx(i) - y(i) ) x(i)

But..,
Convex function일 때만 답을 찾을 수 있음